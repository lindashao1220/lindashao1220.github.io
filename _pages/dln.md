---
title: "Digital Literacy Narratives"
permalink: /dln/
author_profile: false
---

## This is my dln for fall 2023
<img src="/assets/images/dh.jpg" style="zoom:100%"/>

As a 20-year-old university student, I embarked on a new academic adventure this semester: an introductory course in Digital Humanities. Little did I know that it would transform the way I perceive, interact with, and create the digital content. This narrative helps me see the digital part of the world more clearly and explores my evolving digital literacy journey.

Before enrolling in this course, my digital literacy skills were primarily centered around the basics. I knew how to use social media platforms, compose emails, and write research papers using Microsoft Word. My content creation process revolved around the simple tools available on my smartphone, and I hardly ventured beyond that comfort zone. Socially, I interacted with friends and classmates on Instagram, Wechat, and Twitter, but I never delved into the deeper aspects of online communities or digital communication.

In my daily life, I used Google Drive for document storage and Google Calendar for scheduling, but I never truly explored the wealth of digital tools available for research, data management, and analysis. Frankly, I was swimming in the shallow end of the digital pool, unaware of the vast ocean of possibilities that awaited.

Enrolling in the Digital Humanities course was a pivotal moment. The course exposed me to a plethora of digital tools and concepts that have broadened my horizons. I learned about content management systems like Jekyll, which opened up new avenues for blogging and website creation. Markdown language became my go-to for formatting text, and platforms like GitHub allowed me to collaborate with others on coding projects and allow me to do version management.

We delved into the world of APIs. APIs serve as accessible media for retrieving and disseminating data, bridging the gap between different entities and organizations. To gain a deeper understanding of APIs, we started using the Harvard Art Museum's data API, which can be accessed at https://harvardartmuseums.org/collections/api. 
Our exploration began by acquainting ourselves with the API's functionality through the documentation available on their GitHub page: https://github.com/harvardartmuseums/api-docs. With this knowledge, we were able to directly compare and contrast artifacts from various countries in different periods. Specifically, we employed code within the Posit Cloud notebook to perform side-by-side analyses of Indian, Persian, and Turkish artifacts. This allowed us to access details, such as the quantity of artifacts attributed to each of these countries within the Harvard Art Museum's collection. Using the capabilities of the R library, we were able to visualize this data with insightful charts, enabling us to see the disparities in the number of artworks across different nations.  Furthermore, the API empowered us to generate informative bar graphs illustrating the distribution of artworks by accession year, offering a historical perspective on the museum's holdings. To add an extra layer of depth to our analysis, we employed word cloud visualization, which revealed the most frequently occurring words in the artifacts from different countries. This enriched our understanding of the cultural and thematic nuances within the museum's collection.

This all I mentioned is just the start of the class. We have then delved into the text analysis part. We started the journey of reading like a robot and seeing how computers analyze text. Particularly, with optical character recognition (OCR) in Adobe Acrobat, I extracted text from a conversation with Microsoft's Bing Search-Integrated Chatbot, "Jessica Simulation: Love and Loss in the Age of A.I." Narrative, and Conversation Between a Google Engineer and Google's AI Chatbot Lamda. I first read all of them as I normally do, and then ask the computer to read and see its analysis. I have also made some discoveries during this experiment. Firstly, I am surprised by the remarkable speed and efficiency of the computer. It took me 2 hours and more to process and grasp the meaning of those three text blocks. However, when I employed posit cloud code and the Voyant tool for the same task, the processing was completed within seconds, and the data was immediately generated. Secondly, I've observed that this approach assists me in recognizing patterns more effectively. To elaborate, as I read, I become increasingly attuned to the underlying meaning of each sentence. I gradually condense the previously read text into a central idea or background information, which, in turn, aids my overall comprehension. However, the computer is very good at pattern recognition in some sense. It can swiftly identify the most frequently used words in each text, compare them within diverse contexts, and perform clustering and classification across various texts.

In the third part of the class, we delved deeper into the process of mapping data into a spatial representation, as well as explored the capabilities of Microsoft Excel. During this segment, we harnessed the power of geocoding and concatenation functions to manipulate our data effectively. Our approach involved employing ChatGPT to extract the crucial information from the dataset and organize it neatly within an Excel spreadsheet. Subsequently, we leveraged Kepler (available at https://kepler.gl/demo) to visualize and analyze all the data points in a spatial context. This comprehensive workflow allowed us to gain valuable insights from our data and make informed decisions based on the spatial representation.

However, the true eye-opener was when we delved into research data management and analysis. Tools like Zotero and Mendeley revolutionized the way I organize and cite my sources, making research more efficient and organized. We explored data manipulation with Python and data visualization with tools like Tableau, illuminating the power of data-driven insights.

My current abilities have grown significantly, but I am keenly aware of how much more there is to learn. The course has ignited a passion for digital humanities, pushing me to seek further knowledge independently. I now understand that digital literacy is not a static skill but a constantly evolving journey. I aspire to become proficient in data analysis, mastering statistical techniques and data visualization tools to unlock the potential of big data in humanities research.

Furthermore, I am intrigued by the ethical implications of digital humanities. As we navigate the digital landscape, questions about data privacy, biases in algorithms, and the accessibility of digital content are more relevant than ever. I hope to explore these issues deeply and contribute to discussions on responsible and equitable digital practices.

In conclusion, my journey into digital humanities has been a transformative experience. From a basic digital literacy background, I have ventured into the realms of content creation, social interaction, and research data management and analysis. While my current abilities have grown, I am acutely aware of the vast expanse of knowledge yet to be explored. My aspirations include mastering data analysis, understanding the ethical complexities of the digital world, and continuing to adapt and evolve as a digital citizen. This course has opened my eyes to the possibilities and responsibilities that come with digital literacy, and I am excited to continue my journey in this ever-evolving digital wilderness.






<!-- You'll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run `jekyll serve`, which launches a web server and auto-regenerates your site when a file is updated.

To add new posts, simply add a file in the `_posts` directory that follows the convention `YYYY-MM-DD-name-of-post.ext` and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.

Jekyll also offers powerful support for code snippets:

```ruby
def print_hi(name)
  puts "Hi, #{name}"
end
print_hi('Tom')
#=> prints 'Hi, Tom' to STDOUT.
```

Check out the [Jekyll docs][jekyll-docs] for more info on how to get the most out of Jekyll. File all bugs/feature requests at [Jekyllâ€™s GitHub repo][jekyll-gh]. If you have questions, you can ask them on [Jekyll Talk][jekyll-talk].

[jekyll-docs]: https://jekyllrb.com/docs/home
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-talk]: https://talk.jekyllrb.com/ -->
